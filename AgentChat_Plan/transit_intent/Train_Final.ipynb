{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T08:00:57.838552Z",
     "start_time": "2025-07-06T08:00:16.603626Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Joint intent-classification + entity-tagging pipeline\n",
    "(added id2label / label2id so inference shows real tag names)\n",
    "\"\"\"\n",
    "\n",
    "import ast, numpy as np\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, BertForSequenceClassification, BertForTokenClassification,\n",
    "    TrainingArguments, Trainer, DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from seqeval.metrics import f1_score as seq_f1\n",
    "\n",
    "CSV_PATH   = \"TransitChat-Conversational_Route_and_Schedule_Dataset.csv\"\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "INTENT_DIR, SLOT_DIR = \"bert_intent_model\", \"bert_slot_model\"\n",
    "MAX_LEN, EPOCHS, BS, LR, SEED = 128, 3, 16, 2e-5, 42\n",
    "\n",
    "# ───────────────────────── helpers ──────────────────────────\n",
    "def tok_intent(batch, tok):\n",
    "    enc = tok(batch[\"query\"], truncation=True, padding=\"max_length\",\n",
    "              max_length=MAX_LEN)\n",
    "    enc[\"labels\"] = batch[\"intent\"]\n",
    "    return enc\n",
    "\n",
    "def make_slot_labels(example, tok, slot2id):\n",
    "    text = example[\"query\"]\n",
    "    ents = ast.literal_eval(example[\"entities\"])\n",
    "    enc  = tok(text, return_offsets_mapping=True, truncation=True,\n",
    "               padding=\"max_length\", max_length=MAX_LEN)\n",
    "    tags = [\"O\"] * len(enc.input_ids)\n",
    "    lo_text = text.lower()\n",
    "    for t, val in ents.items():\n",
    "        if not val: continue\n",
    "        start = lo_text.find(val.lower())\n",
    "        if start == -1: continue\n",
    "        end = start + len(val)\n",
    "        for i, (s, e) in enumerate(enc.offset_mapping):\n",
    "            if s >= end or e <= start: continue\n",
    "            tags[i] = f\"{'B' if s==start else 'I'}-{t}\"\n",
    "    enc[\"labels\"] = [\n",
    "        slot2id.get(tags[i], slot2id[\"O\"]) if enc.offset_mapping[i]!=(0,0) else -100\n",
    "        for i in range(len(tags))\n",
    "    ]\n",
    "    return {k: v for k, v in enc.items() if k != \"offset_mapping\"}\n",
    "\n",
    "def cls_metrics(pred):\n",
    "    logits, labels = pred\n",
    "    y_pred = logits.argmax(-1)\n",
    "    acc = accuracy_score(labels, y_pred)\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        labels, y_pred, average=\"weighted\", zero_division=0\n",
    "    )\n",
    "    return {\"accuracy\": acc, \"precision\": p, \"recall\": r, \"f1\": f1}\n",
    "\n",
    "def seq_metrics(eval_pred, id2slot):\n",
    "    logits, labels = eval_pred\n",
    "    preds = logits.argmax(-1)\n",
    "    true_tags, pred_tags = [], []\n",
    "    for p_row, l_row in zip(preds, labels):\n",
    "        seq_true, seq_pred = [], []\n",
    "        for p_id, l_id in zip(p_row, l_row):\n",
    "            if l_id == -100:\n",
    "                continue\n",
    "            seq_true.append(id2slot[l_id])\n",
    "            seq_pred.append(id2slot[p_id])\n",
    "        true_tags.append(seq_true)\n",
    "        pred_tags.append(seq_pred)\n",
    "    return {\"f1\": seq_f1(true_tags, pred_tags)}\n",
    "\n",
    "# ───────────────────────── training wrappers ─────────────────\n",
    "def train_intent(ds, id2lbl, lbl2id):\n",
    "    model = BertForSequenceClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(id2lbl),\n",
    "        id2label=id2lbl,\n",
    "        label2id=lbl2id,             # ★ 映射写进 config\n",
    "    )\n",
    "    args = TrainingArguments(\n",
    "        INTENT_DIR, num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BS, per_device_eval_batch_size=BS,\n",
    "        learning_rate=LR, eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "        seed=SEED, load_best_model_at_end=True, metric_for_best_model=\"f1\"\n",
    "    )\n",
    "    Trainer(model=model, args=args,\n",
    "            train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "            compute_metrics=cls_metrics).train()\n",
    "    model.save_pretrained(INTENT_DIR)\n",
    "\n",
    "def train_slots(ds, tok, id2slot, slot2id):\n",
    "    model = BertForTokenClassification.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        num_labels=len(id2slot),\n",
    "        id2label=id2slot,\n",
    "        label2id=slot2id,            # ★ 同样写入\n",
    "    )\n",
    "    args = TrainingArguments(\n",
    "        SLOT_DIR, num_train_epochs=EPOCHS,\n",
    "        per_device_train_batch_size=BS, per_device_eval_batch_size=BS,\n",
    "        learning_rate=LR, eval_strategy=\"epoch\", save_strategy=\"epoch\",\n",
    "        seed=SEED, load_best_model_at_end=True, metric_for_best_model=\"f1\"\n",
    "    )\n",
    "    collator = DataCollatorForTokenClassification(tok)\n",
    "    Trainer(model=model, args=args, data_collator=collator,\n",
    "            train_dataset=ds[\"train\"], eval_dataset=ds[\"validation\"],\n",
    "            compute_metrics=lambda p: seq_metrics(p, id2slot)).train()\n",
    "    model.save_pretrained(SLOT_DIR)\n",
    "\n",
    "# ────────────────────────── main ────────────────────────────\n",
    "def main():\n",
    "    raw = load_dataset(\"csv\", data_files=CSV_PATH)[\"train\"]\n",
    "\n",
    "    # intent label map\n",
    "    intent_lbl = ClassLabel(names=sorted(set(raw[\"intent\"])))\n",
    "    raw = raw.cast_column(\"intent\", intent_lbl)\n",
    "    id2lbl_int = {i: s for i, s in enumerate(intent_lbl.names)}\n",
    "    lbl2id_int = {s: i for i, s in id2lbl_int.items()}\n",
    "\n",
    "    # slot label map (BIO)\n",
    "    slot_types = sorted({k for s in raw[\"entities\"]\n",
    "                         for k in ast.literal_eval(s)})\n",
    "    slot_tags = [\"O\"] + [f\"{io}-{t}\" for t in slot_types for io in (\"B\", \"I\")]\n",
    "    slot2id = {tag: idx for idx, tag in enumerate(slot_tags)}\n",
    "    id2slot = {idx: tag for tag, idx in slot2id.items()}\n",
    "\n",
    "    # split\n",
    "    split = raw.train_test_split(0.2, seed=SEED, stratify_by_column=\"intent\")\n",
    "    val_t = split[\"test\"].train_test_split(0.5, seed=SEED,\n",
    "                                           stratify_by_column=\"intent\")\n",
    "    ds = {\"train\": split[\"train\"],\n",
    "          \"validation\": val_t[\"train\"],\n",
    "          \"test\": val_t[\"test\"]}\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "    ds_int = {k: v.map(lambda b: tok_intent(b, tok), batched=True,\n",
    "                       remove_columns=v.column_names) for k, v in ds.items()}\n",
    "\n",
    "    ds_slot = {k: v.map(lambda ex: make_slot_labels(ex, tok, slot2id),\n",
    "                        batched=False, remove_columns=v.column_names)\n",
    "               for k, v in ds.items()}\n",
    "\n",
    "    train_intent(ds_int, id2lbl_int, lbl2id_int)\n",
    "    tok.save_pretrained(INTENT_DIR)\n",
    "\n",
    "    train_slots(ds_slot, tok, id2slot, slot2id)\n",
    "    tok.save_pretrained(SLOT_DIR)\n",
    "\n",
    "    print(\"✅ finished training both heads\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "id": "4e66ea4ecac99e9a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.225029</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.054017</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033804</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ],
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [75/75 00:17, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.351976</td>\n",
       "      <td>0.819672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.059697</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.033263</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ finished training both heads\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-10T11:23:22.117869Z",
     "start_time": "2025-07-10T11:23:14.824298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "intent_clf = pipeline(\"text-classification\",\n",
    "                      model=\"bert_intent_model\",\n",
    "                      tokenizer=\"bert_intent_model\")\n",
    "\n",
    "slot_tagger = pipeline(\"token-classification\",\n",
    "                       model=\"bert_slot_model\",\n",
    "                       tokenizer=\"bert_slot_model\",\n",
    "                       aggregation_strategy=\"simple\")  # merge B/I spans\n",
    "\n",
    "text = \"Is the train from JFK Airport to San Francisco running next Monday?\"\n",
    "\n",
    "intent_pred  = intent_clf(text)[0]\n",
    "slot_preds   = slot_tagger(text)\n",
    "\n",
    "# Convert slot predictions ➜ dict\n",
    "entities = {p['entity_group'].split('-')[-1]: p['word'] for p in slot_preds}\n",
    "\n",
    "print(intent_pred)   # {'label': 'route_query', 'score': 0.97}\n",
    "print(entities)      # {'source': 'JFK Airport', 'destination': 'Central Park', ...}\n"
   ],
   "id": "d4957203b47a3069",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 'status_query', 'score': 0.8607860803604126}\n",
      "{'transport_mode': 'train', 'source': 'jfk airport', 'destination': 'san francisco', 'date': 'next monday'}\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 可以了，下面是精准度测试",
   "id": "b5bb19149292f874"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-06T08:08:00.439065Z",
     "start_time": "2025-07-06T08:07:54.362235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "evaluate_intent_slot.py  ·  精准度评测脚本（intent + slot）\n",
    "\n",
    "✓ intent 头：accuracy / precision / recall / F1\n",
    "✓ slot  头：seqeval micro-F1 + 明细报告\n",
    "\"\"\"\n",
    "\n",
    "import ast, numpy as np, torch\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from transformers import (\n",
    "    AutoTokenizer, BertForSequenceClassification, BertForTokenClassification,\n",
    "    DataCollatorForTokenClassification\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from seqeval.metrics import classification_report, f1_score as seq_f1\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ─── paths / hparams ───\n",
    "CSV_PATH   = \"TransitChat-Conversational_Route_and_Schedule_Dataset.csv\"\n",
    "INTENT_DIR = \"bert_intent_model\"\n",
    "SLOT_DIR   = \"bert_slot_model\"\n",
    "MAX_LEN    = 128\n",
    "BATCH_SIZE = 32\n",
    "SEED       = 42\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# ─── Load models & label maps ───\n",
    "intent_tok = AutoTokenizer.from_pretrained(INTENT_DIR)\n",
    "slot_tok   = AutoTokenizer.from_pretrained(SLOT_DIR)\n",
    "\n",
    "intent_model = BertForSequenceClassification.from_pretrained(INTENT_DIR)\n",
    "slot_model   = BertForTokenClassification.from_pretrained(SLOT_DIR)\n",
    "\n",
    "id2intent = intent_model.config.id2label\n",
    "intent2id = intent_model.config.label2id\n",
    "id2slot   = slot_model.config.id2label\n",
    "slot2id   = slot_model.config.label2id\n",
    "\n",
    "# ─── Prepare test split (与训练脚本保持一致) ───\n",
    "raw = load_dataset(\"csv\", data_files=CSV_PATH)[\"train\"]\n",
    "\n",
    "# 1️⃣ 把 intent 列转成 ClassLabel，名称顺序必须与模型一致\n",
    "def _intent_name(idx):\n",
    "    if idx in id2intent:          # 键是 int\n",
    "        return id2intent[idx]\n",
    "    return id2intent[str(idx)]    # 键被序列化成 str\n",
    "\n",
    "intent_names = [_intent_name(i) for i in range(len(id2intent))]\n",
    "intent_cl    = ClassLabel(names=intent_names)\n",
    "raw          = raw.cast_column(\"intent\", intent_cl)\n",
    "\n",
    "# 2️⃣ 重现同样的 80-10-10 划分\n",
    "split = raw.train_test_split(0.20, seed=SEED, stratify_by_column=\"intent\")\n",
    "test  = split[\"test\"].train_test_split(0.50, seed=SEED,\n",
    "                                       stratify_by_column=\"intent\")[\"test\"]\n",
    "\n",
    "# ─── Dataset helpers ───\n",
    "def tok_int_batch(batch):\n",
    "    enc = intent_tok(batch[\"query\"], truncation=True, padding=\"max_length\",\n",
    "                     max_length=MAX_LEN)\n",
    "    enc[\"labels\"] = batch[\"intent\"] \n",
    "    return enc\n",
    "\n",
    "def make_slot_labels(ex):\n",
    "    text = ex[\"query\"]\n",
    "    ents = ast.literal_eval(ex[\"entities\"])\n",
    "    enc  = slot_tok(text, return_offsets_mapping=True, truncation=True,\n",
    "                    padding=\"max_length\", max_length=MAX_LEN)\n",
    "    tags = [\"O\"] * len(enc.input_ids)\n",
    "    lo   = text.lower()\n",
    "    for t, val in ents.items():\n",
    "        if not val: continue\n",
    "        start = lo.find(val.lower())\n",
    "        if start == -1: continue\n",
    "        end = start + len(val)\n",
    "        for i, (s, e) in enumerate(enc.offset_mapping):\n",
    "            if s >= end or e <= start: continue\n",
    "            tags[i] = f\"{'B' if s==start else 'I'}-{t}\"\n",
    "    enc[\"labels\"] = [\n",
    "        slot2id.get(tags[i], slot2id[\"O\"]) if enc.offset_mapping[i]!=(0,0) else -100\n",
    "        for i in range(len(tags))\n",
    "    ]\n",
    "    del enc[\"offset_mapping\"]\n",
    "    return enc\n",
    "\n",
    "intent_ds = test.map(tok_int_batch, batched=True,\n",
    "                     remove_columns=test.column_names).with_format(\"torch\")\n",
    "\n",
    "slot_ds   = test.map(make_slot_labels, batched=False,\n",
    "                     remove_columns=test.column_names).with_format(\"torch\")\n",
    "\n",
    "# ─── INTENT evaluation ───\n",
    "int_loader = DataLoader(intent_ds, batch_size=BATCH_SIZE)\n",
    "intent_model.eval()\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for batch in int_loader:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        logits = intent_model(**batch).logits\n",
    "        y_pred.extend(torch.argmax(logits, -1).cpu().tolist())\n",
    "        y_true.extend(labels.cpu().tolist())\n",
    "\n",
    "acc = accuracy_score(y_true, y_pred)\n",
    "p,r,f1,_ = precision_recall_fscore_support(\n",
    "    y_true, y_pred, average=\"weighted\", zero_division=0\n",
    ")\n",
    "\n",
    "# ─── SLOT evaluation ───\n",
    "slot_loader = DataLoader(\n",
    "    slot_ds, batch_size=BATCH_SIZE,\n",
    "    collate_fn=DataCollatorForTokenClassification(slot_tok)\n",
    ")\n",
    "slot_model.eval()\n",
    "true_tags, pred_tags = [], []\n",
    "\n",
    "def _slot_name(idx):\n",
    "    if idx in id2slot:\n",
    "        return id2slot[idx]\n",
    "    return id2slot.get(str(idx), \"O\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in slot_loader:\n",
    "        labels = batch.pop(\"labels\")\n",
    "        mask   = labels != -100\n",
    "        logits = slot_model(**batch).logits\n",
    "        preds  = torch.argmax(logits, -1)\n",
    "        for p_row, l_row, m_row in zip(preds, labels, mask):\n",
    "            t_seq, p_seq = [], []\n",
    "            for pid, lid, m in zip(p_row, l_row, m_row):\n",
    "                if not m: \n",
    "                    continue\n",
    "                t_seq.append(_slot_name(int(lid)))\n",
    "                p_seq.append(_slot_name(int(pid)))\n",
    "            true_tags.append(t_seq)\n",
    "            pred_tags.append(p_seq)\n",
    "\n",
    "slot_f1 = seq_f1(true_tags, pred_tags)\n",
    "\n",
    "# ─── Report ───\n",
    "print(\"──── Intent classification ────\")\n",
    "print(f\"accuracy : {acc:.4f}\")\n",
    "print(f\"precision: {p :.4f}\")\n",
    "print(f\"recall   : {r :.4f}\")\n",
    "print(f\"f1       : {f1:.4f}\")\n",
    "\n",
    "print(\"\\n──── Slot tagging ─────────────\")\n",
    "print(f\"micro-F1 : {slot_f1:.4f}\")\n",
    "print(\"\\nDetailed seqeval report:\")\n",
    "print(classification_report(true_tags, pred_tags))\n"
   ],
   "id": "813895390bdc7672",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "──── Intent classification ────\n",
      "accuracy : 1.0000\n",
      "precision: 1.0000\n",
      "recall   : 1.0000\n",
      "f1       : 1.0000\n",
      "\n",
      "──── Slot tagging ─────────────\n",
      "micro-F1 : 1.0000\n",
      "\n",
      "Detailed seqeval report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "          date       1.00      1.00      1.00        34\n",
      "   destination       1.00      1.00      1.00        50\n",
      "        source       1.00      1.00      1.00        50\n",
      "          time       1.00      1.00      1.00        18\n",
      "transport_mode       1.00      1.00      1.00        50\n",
      "\n",
      "     micro avg       1.00      1.00      1.00       202\n",
      "     macro avg       1.00      1.00      1.00       202\n",
      "  weighted avg       1.00      1.00      1.00       202\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
