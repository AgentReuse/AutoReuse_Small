import autogen
from autogen.agentchat.contrib.retrieve_assistant_agent import RetrieveAssistantAgent
from autogen.agentchat.contrib.retrieve_user_proxy_agent import RetrieveUserProxyAgent
from autogen import AssistantAgent, UserProxyAgent, config_list_from_json, GroupChat, GroupChatManager
import os
from autogen import GroupChat
import json
from autogen.retrieve_utils import TEXT_FORMATS
from Response_reuse import SemanticCache


semantic_cache = SemanticCache(
    embedding_model_path="./m3e-small",
    cache_path="./semantic_cache"
)

######################################################################

config_list_codellama = [
    {
        "base_url": "http://localhost:11434/v1",
        "api_key": "sk-111111111111",
        "model": "llama2:13b"
    }
]
######################################################################

llm_config_codellama={
    "config_list": config_list_codellama,
}
######################################################################
# llm_config_mistral = llm_config_mistral
llm_config_codellama = llm_config_codellama
######################################################################
def is_term(msg):
    try:
        print("TERMINATE Detected")
        return "TERMINATE" in str(msg.get("content","")).strip()
    except Exception:
        return False

output_summarizer = autogen.AssistantAgent(
    name="OutputSummarizer",
    is_termination_msg=is_term,
    system_message="You do not directly engage in communication with other agents. You only need to make a systematic summary of the outputs given by other team members in the current context, which should be organized and easy to understand. ",
    llm_config=llm_config_codellama
)

assistant = autogen.AssistantAgent(
    name="Assistant",
    llm_config=llm_config_codellama,
    # code_execution=False # Disable code execution entirely
    is_termination_msg=is_term,
    code_execution_config={"work_dir":"output/coding", "use_docker":False}
)

coder = autogen.AssistantAgent(
    name="Coder",
    llm_config=llm_config_codellama,
    # code_execution=False # Disable code execution entirely
    is_termination_msg=is_term,
    code_execution_config={"work_dir":"output/coding", "use_docker":False}
)

user_proxy = autogen.UserProxyAgent(
    name="user_proxy",
    human_input_mode="NEVER",
    #human_input_mode="TERMINATE",
    max_consecutive_auto_reply=10,
    code_execution_config={"work_dir": "coding", "use_docker":False},
    llm_config=llm_config_codellama,
    is_termination_msg=is_term,
    system_message="""Reply TERMINATE in the end of your response if the task has been solved at full satisfaction.
Otherwise, reply CONTINUE, or the reason why the task is not solved yet."""
)

task="""
Write a python script to perform a quick sort.
"""

embedding = semantic_cache.get_embedding(task)             #向量化
similar_question, score = semantic_cache.search_similar_query(embedding)   #相似性搜索

response="响应结果"
plan="计划"

isReuse = 0 ## 0为不复用，1为计划复用，2为响应复用

if score<0.75 :
    isReuse=0
elif 0.75 <= score < 0.90:
     isReuse=1
else:
     isReuse=2

if isReuse == 0:
    semantic_cache.save_to_cache(task,response,plan)
elif isReuse == 1:
    semantic_cache.save_to_cache(task,response,plan)
elif isReuse == 2:
    semantic_cache.save_to_cache(task,response,plan)

#task="""
#Write a script to output numbers 1 to X where X is a random number generated by the user_proxy agent
#"""

#user_proxy.initiate_chat(coder, message=task)  # Simple chat with coder
groupchat = autogen.GroupChat(agents=[user_proxy, coder, output_summarizer], messages=[], max_round=12)
manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config_codellama)
user_proxy.initiate_chat(manager, message=task)

# 提取 OutputSummarizer 输出
history = manager.groupchat.messages  # 列表：每条是{"role": "...", "name": "...", "content": "..."}
summary_msgs = [m for m in history if m.get("name") == "OutputSummarizer" and m.get("content")]

if summary_msgs:
    output_summary = summary_msgs[-1]["content"]
    print("=== OutputSummarizer ===\n", output_summary)
    # 如需写文件：
    os.makedirs("output", exist_ok=True)
    with open("output/summary.txt", "w", encoding="utf-8") as f:
        f.write(output_summary)
else:
    print("未在历史中找到 OutputSummarizer 的消息。")
